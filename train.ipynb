{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare Global Variables\n",
    "\n",
    "Define batch size, epochs, input shape, and value of k (for k-fold cross validation) here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of classes\n",
    "classes = ['ChairPose', 'ChestBump', 'ChildPose', 'Dabbing', 'EaglePose', 'HandGun', 'HandShake', 'HighKneel', \n",
    "           'HulkSmash', 'KoreanHeart', 'KungfuCrane', 'KungfuSalute', 'Salute', 'Spiderman', 'WarriorPose']\n",
    "num_classes = len(classes)\n",
    "\n",
    "# Batch size\n",
    "bs = 16\n",
    "\n",
    "# Number of epochs\n",
    "epochs = 100\n",
    "\n",
    "# Image dimensions\n",
    "img_width = 299\n",
    "img_height = 299\n",
    "\n",
    "# All images will be resized to this value\n",
    "img_size = (img_width, img_height)\n",
    "\n",
    "# Here we specify the input shape of our data \n",
    "# This should match the size of images ('img_size') along with the number of channels (3 -> RGB)\n",
    "input_shape = (img_width, img_height, 3)\n",
    "\n",
    "# k-fold cross validation\n",
    "# Set k=1 for normal train-val split\n",
    "k = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve external libraries\n",
    "\n",
    "2 external libraries are used.\n",
    "<br>\n",
    "1) AdaBound for Keras (https://arxiv.org/abs/1902.09843)\n",
    "<br>\n",
    "2) Augmentor (http://augmentor.readthedocs.io)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBound (optimiser)\n",
    "!git clone https://github.com/titu1994/keras-adabound.git\n",
    "!cp keras-adabound/adabound.py .\n",
    "\n",
    "# Augmentor (data augmentation)\n",
    "!pip install Augmentor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and Explore the Dataset\n",
    "\n",
    "The contents of the `.zip` (no longer accessible) are extracted to the base directory, which contains `train` and `val` subdirectories. The folders have the following structure:\n",
    "\n",
    "```\n",
    "---------------\n",
    "train\n",
    "|- ChairPose\n",
    "|- ChestBump\n",
    "|- ChildPose\n",
    "|- Dabbing\n",
    "|- EaglePose\n",
    "|- HandGun\n",
    "|- HandShake\n",
    "|- HighKneel\n",
    "|- HulkSmash\n",
    "|- KoreanHeart\n",
    "|- KungfuCrane\n",
    "|- KungfuSalute\n",
    "|- Salute\n",
    "|- Spiderman\n",
    "|- WarriorPose\n",
    "\n",
    "val\n",
    "|- ChairPose\n",
    "|- ChestBump\n",
    "|- ChildPose\n",
    "|- Dabbing\n",
    "|- EaglePose\n",
    "|- HandGun\n",
    "|- HandShake\n",
    "|- HighKneel\n",
    "|- HulkSmash\n",
    "|- KoreanHeart\n",
    "|- KungfuCrane\n",
    "|- KungfuSalute\n",
    "|- Salute\n",
    "|- Spiderman\n",
    "|- WarriorPose\n",
    "---------------\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf data && rm -rf model && rm -rf graph\n",
    "# Creating two directories - \"data\" and \"data/trainset_11classes_0_00000\"\n",
    "!mkdir data && mkdir data/trainset_11classes_0_00000 && mkdir model && mkdir graph\n",
    "# Downloading the ai-camp competition dataset\n",
    "# !wget -N https://ai-camp.s3-us-west-2.amazonaws.com/trainset_11classes_0_00000.zip\n",
    "# Unzip the data into the folder \"data/trainset_11classes_0_00000\"\n",
    "!unzip -qq -n trainset_11classes_0_00000.zip -d data/trainset_11classes_0_00000\n",
    "# Switch directory to \"data/trainset_11classes_0_00000\" and show its content\n",
    "!cd data/trainset_11classes_0_00000 && ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "base_dir = 'data/trainset_11classes_0_00000'\n",
    "\n",
    "# Directory to our training data\n",
    "train_folder = os.path.join(base_dir, 'train')\n",
    "\n",
    "# Directory to our validation data\n",
    "val_folder = os.path.join(base_dir, 'val')\n",
    "\n",
    "# Directory to our model(s)\n",
    "model_folder = 'model/'\n",
    "\n",
    "# Directory to our graph(s)\n",
    "graph_folder = 'graph/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's find out the total number of images in each `train` and `val`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List folders and number of files\n",
    "print(\"Directory, Number of files\")\n",
    "for root, subdirs, files in os.walk(base_dir):\n",
    "    print(root, len(files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are 15 categories/folders in each `train` and `val` folder.\n",
    "\n",
    "Now let's take a look at a few images to get a better sense of what the `KoreanHeart` and `KungfuCrane` categories look like. First, configure the matplotlib parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# Parameters for our graph; we'll output images in a 4x4 configuration\n",
    "nrows = 4\n",
    "ncols = 4\n",
    "\n",
    "# Index for iterating over images\n",
    "pic_index = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, display a batch of 8 KoreanHeart and 8 KungfuCrane poses. You can rerun the cell to see a new batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Path to KoreanHeart and KungfuCrane\n",
    "train_koreanheart_dir= \"data/trainset_11classes_0_00000/train/KoreanHeart\"\n",
    "train_kungfucrane_dir= \"data/trainset_11classes_0_00000/train/KungfuCrane\"\n",
    "train_koreanheart_fnames = os.listdir(train_koreanheart_dir)\n",
    "train_kungfucrane_fnames = os.listdir(train_kungfucrane_dir)\n",
    "\n",
    "# Set up matplotlib fig, and size it to fit 4x4 pics\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 15)\n",
    "\n",
    "pic_index += 8\n",
    "next_koreanheart_pix = [os.path.join(train_koreanheart_dir, fname) \n",
    "                for fname in train_koreanheart_fnames[pic_index-8:pic_index]]\n",
    "next_kungfucrane_pix = [os.path.join(train_kungfucrane_dir, fname) \n",
    "                for fname in train_kungfucrane_fnames[pic_index-8:pic_index]]\n",
    "\n",
    "for i, img_path in enumerate(next_koreanheart_pix+next_kungfucrane_pix):\n",
    "    # Set up subplot; subplot indices start at 1\n",
    "    sp = plt.subplot(nrows, ncols, i + 1)\n",
    "    sp.axis('Off') # Don't show axes (or gridlines)\n",
    "    \n",
    "    img = mpimg.imread(img_path)\n",
    "    plt.imshow(img)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise `train` folder by moving all images from `val` folder to `train` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Move all images from val folder to train folder\n",
    "def init_train():\n",
    "    for root, subdirs, files in os.walk(base_dir):\n",
    "        if (root.split('/')[-2] == 'val'):\n",
    "            cls = root.split('/')[-1]\n",
    "            src_dir = root\n",
    "            dest_dir = os.path.join(train_folder, cls)\n",
    "            \n",
    "            for f in os.listdir(src_dir):\n",
    "                shutil.move(os.path.join(src_dir, f), dest_dir)\n",
    "                \n",
    "init_train()\n",
    "\n",
    "# Sanity check\n",
    "# Should see 0 images in all subdirs of data/trainset_11classes_0_00000/val\n",
    "print(\"Directory, Number of files\")\n",
    "for root, subdirs, files in os.walk(base_dir):\n",
    "    print(root, len(files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create k groups from images in each class in `train` folder (for k-fold cross validation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# l - list, n - size of chunks\n",
    "def chunks(l, n):\n",
    "    # For item i in a range that is a length of l,\n",
    "    for i in range(0, len(l), n):\n",
    "        # Create an index range for l of n items:\n",
    "        yield l[i:i+n]\n",
    "\n",
    "list_dict = []\n",
    "for x in range(num_classes):\n",
    "    images = os.listdir(os.path.join(train_folder, classes[x]))\n",
    "    random.shuffle(images)\n",
    "    num_val = int(len(images)/k) if k > 1 else int(len(images)*0.2)\n",
    "    \n",
    "    i = 0\n",
    "    dict = {}\n",
    "    \n",
    "    for l in list(chunks(images, num_val)):\n",
    "        for image in l:\n",
    "            dict[image] = i\n",
    "        i += 1\n",
    "        if (i >= k): break\n",
    "\n",
    "    list_dict.append(dict)\n",
    "    \n",
    "# Sanity check\n",
    "# Should see all images mapped to an index < k for all classes (except when k = 1)\n",
    "for i, d in enumerate(list_dict):\n",
    "    print('\\033[1m' + str(i+1) + '. ' + classes[i] + '\\033[0m')\n",
    "    print(d)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise `val` folder by moving designated (index) images from `train` folder to `val` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move designated images from train folder to val folder\n",
    "def init_val(index):\n",
    "    init_train()\n",
    "    \n",
    "    for x in range(num_classes):\n",
    "        src_dir = os.path.join(train_folder, classes[x])\n",
    "        dest_dir = os.path.join(val_folder, classes[x])\n",
    "\n",
    "        for image, n in list_dict[x].items():\n",
    "            if (n == index):\n",
    "                shutil.move(os.path.join(src_dir, image), dest_dir)\n",
    "        \n",
    "    for root, subdirs, files in os.walk(base_dir):\n",
    "        print(root, len(files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Let's set up data generators that will read images from our source folders and convert them to float32 tensors. We'll have one generator for each training and validation folders.\n",
    "\n",
    "### Batch\n",
    "Our generators will yield batches of `32` images of size `299 x 299` and their labels.\n",
    "\n",
    "### Feature scaling\n",
    "Recall that in our MNIST/CIFAR-10 exercises, data that goes into a neural network should be normalised in a way that is easier to be processed by the network. In our case, we will preprocess our images by normalising the pixels values to be in the 0 to 1 range. This happens by dividing each pixel value by 255 and this process is known as data normalisation or rescaling.\n",
    "\n",
    "### Generator - ImageDataGenerator\n",
    "To rescale the data, we use `keras.preprocessing.image.ImageDataGenerator` class with the `rescale` parameter. This class will also allow us to instantiate generators of augmented image batches (and their labels) via `.flow_from_directory(directory)`. These generators can then be used with the Keras model methods that accept data generators as inputs such as `fit_generator`, `evaluate_generator` and `predict_generator`. We used data augmentation for the training image generator. To find out more about how to do image augmentation in keras, go [here](https://keras.io/preprocessing/image/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import Augmentor\n",
    "\n",
    "p = None\n",
    "\n",
    "def augment_data():\n",
    "    global p\n",
    "    p = Augmentor.Pipeline(train_folder)\n",
    "    p.resize(probability=1, width=299, height=299)\n",
    "    p.random_distortion(probability=0.5, grid_width=4, grid_height=4, magnitude=7)\n",
    "    p.gaussian_distortion(probability=0.5, grid_width=4, grid_height=4, magnitude=7,corner='bell',method='in')\n",
    "    p.rotate(probability=0.5, max_left_rotation=15, max_right_rotation=15)\n",
    "    p.flip_left_right(probability=0.5)\n",
    "    p.skew(probability=0.5, magnitude=0.25)\n",
    "    p.shear(probability=0.5, max_shear_left=15, max_shear_right=15)\n",
    "    p.random_brightness(probability=0.5, min_factor=0.5, max_factor=1.5)\n",
    "    p.random_contrast(probability=0.5, min_factor=0.75, max_factor=1.25)\n",
    "    p.status() # sanity check\n",
    "    \n",
    "    print(\"Preparing generator for training dataset\")\n",
    "    train_generator = p.keras_generator(batch_size=bs)\n",
    "    \n",
    "    val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    # Flow validation images in batches of bs using val_datagen generator\n",
    "    print(\"Preparing generator for validation dataset\")\n",
    "    val_generator= val_datagen.flow_from_directory(\n",
    "        directory= val_folder, \n",
    "        target_size=img_size,\n",
    "        batch_size=bs,\n",
    "        class_mode='categorical')\n",
    "    \n",
    "    return train_generator, val_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building our CNN Model\n",
    "\n",
    "The images that will go into our convnet are **299 x 299** color (RGB) images.\n",
    "\n",
    "Here, we made use of transfer learning. In transfer learning, we take the pre-trained weights of an already trained model (one that has been trained on millions of images belonging to thousands of classes) and use these already learned features to predict new classes in our dataset. We concluded, through multiple experiments, that Xception (by Google) gives us the highest accuracy in classifying our dataset. \n",
    "\n",
    "For the classifier block, we went with a [global average pooling layer](https://alexisbcook.github.io/2017/global-average-pooling-layers-for-object-localization/). We also decided not to use a series of dense layers but instead went straight to the final softmax classification layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.mobilenet_v2 import MobileNetV2\n",
    "from keras.applications.xception import Xception\n",
    "from keras.applications.densenet import DenseNet201\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.applications.nasnet import NASNetLarge\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, BatchNormalization, Activation, GlobalAveragePooling2D, Dense, Dropout, MaxPooling2D, Flatten\n",
    "\n",
    "def build_model():\n",
    "    print('Loading model and pre-trained weights...')\n",
    "    # Specify conv layers\n",
    "    base_model = Xception(include_top=False, weights='imagenet', pooling=None, input_shape=input_shape)\n",
    "\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Blocks 11, 12, 13, 14 to be trained\n",
    "    for x in range(-36, 0, 1):\n",
    "        base_model.layers[x].trainable = True\n",
    "            \n",
    "    # Specify classifier layers (experiment)\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    return Model(inputs=base_model.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will configure the specifications for model training.\n",
    "\n",
    "We train our model with `categorical_crossentropy` loss, because this is a multi-class problem. We will use the `AdaBound` optimizer with default settings. During training, we want to monitor `accuracy` of the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "from adabound import AdaBound\n",
    "\n",
    "def compile_model(model):\n",
    "    opt = AdaBound(lr=1e-03,\n",
    "                   final_lr=0.1,\n",
    "                   gamma=1e-03,\n",
    "                   weight_decay=0.,\n",
    "                   amsbound=False)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Checkpoints\n",
    "\n",
    "Let's setup a [checkpoint](https://keras.io/callbacks/) to help us monitor the validation accuracy as the model trains. This checkpoint will save the model with best validation accuracy seen so far.\n",
    "\n",
    "## Model Training \n",
    "\n",
    "Let's train on all the images in the training set, and validate against all validation images.\n",
    "\n",
    "Note: This may take a while to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "def train_model(model, train_generator, val_generator, index):\n",
    "    # Set up checkpoints\n",
    "    bestValidationCheckpointer = ModelCheckpoint(model_folder + 'train_model_' + str(index) + '.hdf5', \n",
    "                                                 monitor='val_acc', \n",
    "                                                 save_best_only=True, \n",
    "                                                 verbose=1)\n",
    "    \n",
    "    earlyStopper = EarlyStopping(monitor='val_acc', \n",
    "                                 patience=25,\n",
    "                                 verbose=1)\n",
    "\n",
    "    history = model.fit_generator(\n",
    "          train_generator,\n",
    "          steps_per_epoch=len(p.augmentor_images) // bs + 1, # train_generator.samples // bs + 1,\n",
    "          epochs=epochs,\n",
    "          validation_data=val_generator,\n",
    "          validation_steps=val_generator.samples // bs + 1,\n",
    "          callbacks=[bestValidationCheckpointer, earlyStopper])\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphing Accuracy and Loss Functions\n",
    "\n",
    "Plot accuracy and loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_graphs(history, index):\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    ep = range(1, len(acc)+1)\n",
    "\n",
    "    plt.plot(ep, acc, 'bo', label='Training accuracy')\n",
    "    plt.plot(ep, val_acc, 'b', label='Validation accuracy')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.savefig(graph_folder + 'train_acc_' + str(index) + '.png')\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(ep, loss, 'bo', label='Training loss')\n",
    "    plt.plot(ep, val_loss, 'b', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(graph_folder + 'train_loss_' + str(index) + '.png')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Accuracy and Loss of the Model\n",
    "\n",
    "With a trained model, we can evaluate the model performance against the truth labels of our validation set. First, we load the best model encountered during training.\n",
    "\n",
    "Then, we validate accuracy of the loaded model on our good old validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "def evaluate_model(val_generator, index):\n",
    "    file = model_folder + 'train_model_' + str(index) + '.hdf5'\n",
    "    model = load_model(file, custom_objects={'AdaBound': AdaBound})\n",
    "\n",
    "    val_generator.reset()\n",
    "    scores = model.evaluate_generator(val_generator, steps=val_generator.samples // val_generator.batch_size + 1, verbose=1)\n",
    "    os.rename(file, model_folder + 'train_model_' + str(index) + '_' + str(scores[1]) + '.hdf5')\n",
    "    \n",
    "    return scores[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commence Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "avg_acc = 0\n",
    "\n",
    "for x in range(k):\n",
    "    init_val(x)\n",
    "    train_gen, val_gen = augment_data()\n",
    "    model = build_model()\n",
    "    model.summary()\n",
    "    compile_model(model)\n",
    "    history = train_model(model, train_gen, val_gen, x)\n",
    "    plot_graphs(history, x)\n",
    "    avg_acc += evaluate_model(val_gen, x)\n",
    "\n",
    "avg_acc /= k\n",
    "print('\\033[1m' + 'Overall model\\'s accuracy: ' + str(avg_acc) + '\\033[0m')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
